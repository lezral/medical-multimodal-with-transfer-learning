{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cnn_model\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adadelta, Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras import backend as K\n",
    "from keras.layers import Merge\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, Flatten, Conv2D, MaxPooling1D, MaxPooling2D, UpSampling2D, BatchNormalization, Activation, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "This section focuses on the multimodal\n",
    "architecture which consists of a text submodel and\n",
    "an image submodel with an emphisis on transfer learning. The text submodel is developed by taking the network layers, from input layers to feature vectors, from a trained text classifier. Similarly, the image sub-model takes the network layers of a trained image classifier from input layers to feature vectors.\n",
    "\n",
    "Then the encoded text and image feature vectors are concatenated into a single flat feature vector.\n",
    "This feature vector is then passed onto a densely connected decoder for the binary classification. Applying transfer learning to the two encoders makes this multimodal function under small data situations such as this one (below 3K). The transferred text and image encoders are the respective pre-trained embedding and residual layers. \n",
    "\n",
    "pretrained encoders are then finely tuned on low learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/text_processed.pkl', 'rb') as handle:\n",
    "    text = pickle.load(handle)\n",
    "    \n",
    "with open('data/x_ray_processed.pkl', 'rb') as handle:\n",
    "    img = pickle.load(handle)\n",
    "    \n",
    "with open('data/vocab.json', 'r') as voc:\n",
    "    vocab = json.load(voc) \n",
    "    \n",
    "original_data = pd.read_csv('data/ids_raw_texts_labels.csv')\n",
    "# taking the intersection of ids of both datasets\n",
    "ids   = list(set(list(text.keys())) & set(list(img.keys())))\n",
    "text  = [text[patient] for patient in ids]\n",
    "img   = [img[patient] for patient in ids]\n",
    "y     = [original_data[original_data['ID'] == patient].Labels.item() for patient in ids]\n",
    "\n",
    "# Split the dataset for text\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset for img\n",
    "X_train_img, X_test_img, y_train, y_test = train_test_split(img, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# random state is the same so same id splits go to both types of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text CNN\n",
    "\n",
    "We choose a CNN based text classifier (Kim,2015) to meet this task. We also extend the 1-D CNN for classifying short e-commerce product descriptions (Eskesen, 2017) to our text classifier. Resemblance in structure of product descriptions, features of short radiology reports (27.4 words per case on average) are extracted by the Word2Vec approach (Mikolov, 2013).\n",
    "\n",
    "The transfered embedding layer is a pretrained word2vec model fine-tuned on a large medical corpus. The word embeddings can be found under 'data/new_data_embed300.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING\n",
    "MAX_NUM_WORDS  = 15000\n",
    "EMBEDDING_DIM  = 300\n",
    "MAX_SEQ_LENGTH = 140\n",
    "USE_GLOVE      = True\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [3,4,5]\n",
    "FEATURE_MAPS   = [10,10,10]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 200\n",
    "NB_EPOCHS      = 10\n",
    "RUNS           = 1\n",
    "\n",
    "\n",
    "def create_glove_embeddings():\n",
    "    embeddings_index = {}\n",
    "    f = open('data/new_data_embed300.txt')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "    for word, i in vocab.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM,\n",
    "                     input_length=MAX_SEQ_LENGTH,\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "\n",
    "for i in range(RUNS):\n",
    "    print('Running iteration %i/%i' % (i+1, RUNS))\n",
    "        \n",
    "    emb_layer = None\n",
    "    if USE_GLOVE:\n",
    "        emb_layer = create_glove_embeddings()\n",
    "    \n",
    "    model = cnn_model.build_cnn(\n",
    "        embedding_layer=emb_layer,\n",
    "        num_words=MAX_NUM_WORDS,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        filter_sizes=FILTER_SIZES,\n",
    "        feature_maps=FEATURE_MAPS,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adadelta(clipvalue=3),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        np.array(X_train_text), y_train,\n",
    "        epochs=NB_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        validation_data=(np.array(X_test_text), y_test),\n",
    "        callbacks=[ModelCheckpoint('best_models/text_model-%i.h5'%(i+1), monitor='val_loss',\n",
    "                                   verbose=1, save_best_only=True, mode='min'),\n",
    "                   ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01)\n",
    "                  ]\n",
    "    )\n",
    "    print()\n",
    "    histories.append(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image DenseNet121\n",
    "\n",
    "The image submodal is centered around the idea of transfer learning. CNN encoders pre-trained on ImageNet and National Health Institutes ChestX-ray14 gave several different experimentation combinations with diferent encoders (VGG, DenseNet, ResNet, etc.). The DenseNet121 (He et al., 2015) pre-trained on ChestX-ray14 was chosen as the encoder due to its superior accuracy. This encoder is then fed into a simple batch normalization which is then fed into a decoder for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape=(224, 224, 3))\n",
    "base_model = DenseNet121(include_top=True, weights='best_models/CheXNet_Densenet121_weights.h5', input_tensor=image_input, input_shape=None, pooling=None, classes=14)\n",
    "last_layer = base_model.get_layer('avg_pool').output\n",
    "x = BatchNormalization()(last_layer)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Dense(1, activation='softmax')(x) \n",
    "model = Model(image_input , out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=Adam(), metrics=['accuracy'])    \n",
    "hist = model.fit(np.array(X_train_img), np.array(y_train), batch_size=32, epochs=3, verbose=1, validation_data=(np.array(X_test_img), np.array(y_test)))\n",
    "#model.save_weights('best_models/img_model-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal\n",
    "\n",
    "Below one can see how the multimodal is created. It is explicitly reconstructed for explanatory purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Text Sub-model\n",
    "def create_channel(x, filter_size, feature_map):\n",
    "    x = Conv1D(feature_map, kernel_size=filter_size, activation='relu', strides=1,\n",
    "               padding='same', kernel_regularizer=regularizers.l2(0.03))(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=1, padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    return x\n",
    "x_in = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "channels = []\n",
    "embedding_layer = create_glove_embeddings()\n",
    "emb_layer = embedding_layer(x_in)\n",
    "if DROPOUT_RATE:\n",
    "    emb_layer = Dropout(DROPOUT_RATE)(emb_layer)\n",
    "for ix in range(len(FILTER_SIZES)):\n",
    "    x = create_channel(emb_layer, FILTER_SIZES[ix], FEATURE_MAPS[ix])\n",
    "    channels.append(x)\n",
    "# Concatenate all channels\n",
    "x = concatenate(channels)\n",
    "text_last_layer = concatenate(channels)\n",
    "###########################################\n",
    "# Image Sub-model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "base_model = DenseNet121(include_top=True, weights='best_models/CheXNet_Densenet121_weights.h5', input_tensor=image_input, input_shape=None, pooling=None, classes=14)\n",
    "last_layer = base_model.get_layer('avg_pool').output\n",
    "img_last_layer = BatchNormalization()(last_layer)\n",
    "###########################################\n",
    "# Fusion\n",
    "fusion = concatenate([text_last_layer,img_last_layer])\n",
    "x = BatchNormalization()(fusion)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Dense(1, activation='softmax')(x)\n",
    "multi_model = Model([x_in, image_input] , out)\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# multi_model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])    \n",
    "hist = multi_model.fit(x = ([np.array(X_train_text),np.array(X_train_img)]), y = np.array(y_train), batch_size=32, epochs=3, verbose=1, validation_data=(([np.array(X_test_text),np.array(X_test_img)]), np.array(y_test)))\n",
    "#model.save_weights('best_models/multi_model-1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
